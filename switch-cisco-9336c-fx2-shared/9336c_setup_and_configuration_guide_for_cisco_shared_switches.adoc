---
sidebar: sidebar
permalink: switch-cisco-9336c-fx2-shared/9336c_setup_and_configuration_guide_for_cisco_shared_switches.html
keywords:
summary:
---

= Setup and install Cisco shared switches
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
Learn how to setup and install Cisco shared switches.

.Before you begin
* You must have the required shared switch documentation.
+
See <<Required documentation for shared switches>> for more information.
* You must have the required controller documentation and ONTAP documentation.
+
See https://docs.netapp.com/us-en/ontap/index.html[NetApp ONTAP documentation].

* You must have the applicable licenses, network and configuration information, and cables.

* You must have the completed cabling worksheets.
+
For more information on cabling, refer to the https://hwu.netapp.com[Hardware Universe].

== Set up shared switches
All Cisco shared switches arrive with the standard Cisco factory-default configuration. These switches also have the current version of the NX-OS software but do not have the RCFs loaded.

IMPORTANT: You must download the applicable NetApp RCFs from the https://mysupport.netapp.com[NetApp Support Site] for the switches that you receive.

.Steps
. Rack the switches, controllers and NS224 NVMe storage shelves.
+
See the
https://docs.netapp.com/platstor/topic/com.netapp.doc.hw-sw-9336c-install-cabinet/GUID-92287262-E7A6-4A62-B159-7F148097B33B.html[Racking instructions] to learn how to rack the switch in a NetApp cabinet.
// andris /ontap-systems-switches/pull/30
. Power on the switches, controllers and NS224 NVMe storage shelves.
[start=3]
. [[step3]]Perform an initial configuration of the switches.
+
For configuration, you need the appropriate number and type of cables and cable connectors for your switches. Depending on the type of switch you are initially configuring, you need to connect to the switch console port with the included console cable; you also need to provide specific network information.
+
. Verify the configuration choices you made in the display that appears at the end of the setup, and make sure that you save the configuration.
. Check the software version on the switches, and if necessary, download the NetApp-supported version of the software to the switches.
+
If you download the NetApp-supported version of the software, then you must also download the NetApp Network Switch Reference Configuration File and merge it with the configuration you saved in <<step3,Step 3>>.
+
You can download the file and the instructions from the https://mysupport.netapp.com/site/info/cisco-ethernet-switch[Cisco Ethernet Switches] page.
+
If you have your own switches, refer to the http://www.cisco.com[Cisco] site.

== Install switches
The examples in this procedure use two nodes. These nodes use two 100GbE cluster interconnect ports e3a and e3b, as per the A400 controller.
See the https://hwu.netapp.com[Hardware Universe] to verify the correct cluster ports on your platforms.

NOTE: The command outputs might vary depending on different releases of ONTAP.

.Switch and node nomenclature
The examples in this procedure use the following switch and node nomenclature:

* The names of the two Cisco switches are _cs1_ and _cs2_.
* The node names are _cluster1-01_ and _cluster1-02_.
* The cluster LIF names are _cluster1-01_clus1_ and _cluster1-01_clus2_ for cluster1-01 and _cluster1-02_clus1_ and _cluster1-02_clus2_ for cluster1-02.
* The cluster1::*> prompt indicates the name of the cluster.

[NOTE]
====
* The procedure requires the use of both ONTAP commands and Cisco Nexus 9000 Series Switches commands; ONTAP commands are used unless otherwise indicated.
* Before you perform this procedure, make sure that you have a current backup of the switch configuration.
====

.Steps
[start=1]
. [[step1]]If AutoSupport is enabled on this cluster, suppress automatic case creation by invoking an AutoSupport message: `system node autosupport invoke -node * -type all -message MAINT=x h`
+
Where x is the duration of the maintenance window in hours.

[start=2]
. [[step2]]Change the privilege level to advanced, entering y when prompted to continue:
`set -privilege advanced`
+
The advanced prompt (*>) appears.
[start=3]
. [[step3]]Display how many cluster interconnect interfaces are configured in each node for each cluster interconnect switch:
`network device-discovery show -protocol cdp`
+
[subs=+quotes]
----
cluster1:: *network device-discovery show -protocol cdp*
Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface         Platform
----------- ------ ------------------------- ----------------- --------
cluster1-02/cdp
            e3a    cs1                       Eth1/2            N9K-C9336C
            e3b    cs2                       Eth1/2            N9K-C9336C
cluster1-01/cdp
            e3a    cs1                       Eth1/1            N9K-C9336C
            e3b    cs2                       Eth1/1            N9K-C9336C
4 entries were displayed.
----
[start=4]
. [[step4]]Check the administrative or operational status of each cluster interface:
.. Display the network port attributes:
`network port show –ipspace Cluster`
+
[subs=+quotes]
----
cluster1::*> *network port show -ipspace Cluster*
Node: cluster1-02
                                                  Speed(Mbps)  Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper   Status
--------- ------------ ---------------- ---- ---- ------------ ------
e3a       Cluster      Cluster          up   9000  auto/100000 healthy
e3b       Cluster      Cluster          up   9000  auto/100000 healthy

Node: cluster1-01
                                                  Speed(Mbps)  Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper   Status
--------- ------------ ---------------- ---- ---- ------------ ------
e3a       Cluster      Cluster          up   9000  auto/100000 healthy
e3b       Cluster      Cluster          up   9000  auto/100000 healthy
4 entries were displayed.
----
[start=2]
.. Display information about the LIFs:
`network interface show - vserver Cluster`
+
[subs=+quotes]
----
cluster1::*> *network interface show -vserver Cluster*
        Logical            Status     Network            Current    Current Is
Vserver Interface          Admin/Oper Address/Mask       Node        Port   Home
------- ------------------ ---------- ------------------ ----------- ------ ----
Cluster
        cluster1-01_clus1  up/up      169.254.209.69/16  cluster1-01  e3a   true
        cluster1-01_clus2  up/up      169.254.49.125/16  cluster1-01  e3b   true
        cluster1-02_clus1  up/up      169.254.47.194/16  cluster1-02  e3a   true
        cluster1-02_clus2  up/up      169.254.19.183/16  cluster1-02  e3b   true
4 entries were displayed.
----
[start=5]
. [[step5]]Ping the remote cluster LIFs:
`cluster ping-cluster -node node-name`
+
[subs=+quotes]
----
cluster1::*> *cluster ping-cluster -node cluster1-02*
Host is cluster1-02
Getting addresses from network interface table...
Cluster cluster1-01_clus1 169.254.209.69 cluster1-01     e3a
Cluster cluster1-01_clus2 169.254.49.125 cluster1-01     e3b
Cluster cluster1-02_clus1 169.254.47.194 cluster1-02     e3a
Cluster cluster1-02_clus2 169.254.19.183 cluster1-02     e3b
Local = 169.254.47.194 169.254.19.183
Remote = 169.254.209.69 169.254.49.125
Cluster Vserver Id = 4294967293
Ping status:
....
Basic connectivity succeeds on 4 path(s)
Basic connectivity fails on 0 path(s)
................
Detected 9000 byte MTU on 4 path(s):
    Local 169.254.19.183 to Remote 169.254.209.69
    Local 169.254.19.183 to Remote 169.254.49.125
    Local 169.254.47.194 to Remote 169.254.209.69
    Local 169.254.47.194 to Remote 169.254.49.125
Larger than PMTU communication succeeds on 4 path(s)
RPC status:
2 paths up, 0 paths down (tcp check)
2 paths up, 0 paths down (udp check)
----
[start=6]
. [[step6]]Verify that the auto-revert command is enabled on all cluster LIFs:
`network interface show - vserver Cluster -fields auto-revert`
+
[subs=+quotes]
----
cluster1::*> *network interface show -vserver Cluster -fields auto-revert*
          Logical
Vserver   Interface            Auto-revert
--------- ––––––-------------- ------------
Cluster
          cluster1-01_clus1    true
          cluster1-01_clus2    true
          cluster1-02_clus1    true
          cluster1-02_clus2    true
4 entries were displayed.
----
[start=7]
. [[step7]]Enable the Ethernet switch health monitor log collection feature for collecting switch-related log files, using the following commands:
+
** `system switch ethernet log setup-password`
** `system switch ethernet log enable-collection`
+
[subs=+quotes]
----
cluster1::*> *system switch ethernet log setup password*
Enter the switch name: <return>
The switch name entered is not recognized.
Choose from the following list:
cs1
cs2
cluster1::*> system switch ethernet log setup-password
Enter the switch name: cs1
RSA key fingerprint is e5:8b:c6:dc:e2:18:18:09:36:63:d9:63:dd:03:d9:cc
Do you want to continue? {y|n}::[n] y
Enter the password: <enter switch password>
Enter the password again: <enter switch password>
cluster1::*> system switch ethernet log setup-password
Enter the switch name: cs2
RSA key fingerprint is 57:49:86:a1:b9:80:6a:61:9a:86:8e:3c:e3:b7:1f:b1
Do you want to continue? {y|n}:: [n] y
Enter the password: <enter switch password>
Enter the password again: <enter switch password>
cluster1::*> system  switch ethernet log enable-collection
Do you want to enable cluster log collection for all nodes in the cluster? {y|n}: [n] y
Enabling cluster switch log collection.
cluster1::*>
----

[NOTE]
If any of these commands return an error, contact NetApp support.
