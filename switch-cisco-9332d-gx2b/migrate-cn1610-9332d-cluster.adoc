---
permalink: switch-cisco-9332d-gx2b/migrate-cn1610-9332d-cluster.html
sidebar: sidebar
keywords: migrate cluster Cisco Nexus 9332D-GX2B cluster switches cn1610
summary: 'You can migrate nondisruptively NetApp CN1610 cluster switches for an ONTAP cluster to Cisco Nexus 9332D-GX2B cluster network switches.'
---
= Migrate from a NetApp CN1610 cluster switch to a Cisco 9332D-GX2B cluster switch
:icons: font
:imagesdir: ../media/

[.lead]
You can migrate NetApp CN1610 cluster switches for an ONTAP cluster to Cisco 9332D-GX2B cluster switches. This is a nondisruptive procedure.

== Review requirements

You must be aware of certain configuration information, port connections and cabling requirements when you are replacing NetApp CN1610 cluster switches with Cisco 9332D-GX2B cluster switches. Also, verify the switch serial number to ensure that the correct switch is migrated.

.Supported switches

The following cluster switches are supported:

* NetApp CN1610
* Cisco 9332D-GX2B

For details of supported ports and their configurations, see the https://hwu.netapp.com/[Hardware Universe^].

.What you'll need
Verify that your configuration meets the following requirements:

* The existing cluster is correctly set up and functioning.
* All cluster ports are in the *up* state to ensure nondisruptive operations.
* The Cisco 9332D-GX2B cluster switches are configured and operating under the correct version of NX-OS installed with the reference configuration file (RCF) applied.
* The existing cluster network configuration has the following:
** A redundant and fully functional NetApp cluster using NetApp CN1610 switches.
** Management connectivity and console access to both the NetApp CN1610 switches and the new switches.
** All cluster LIFs in the up state with the cluster LIFs are on their home ports.
//** ISL ports enabled and cabled between the NetApp CN1610 switches and between the new switches.
* Some of the ports are configured on Cisco 9332D-GX2B switches to run at 40GbE or 100GbE.
* You have planned, migrated, and documented 40GbE and 100GbE connectivity from nodes to Cisco 9332D-GX2B cluster switches.

== Migrate the switches

.About the examples

The examples in this procedure use the following switch and node nomenclature:

* The existing CN1610 cluster switches are _C1_ and _C2_.
* The new 9332D-GX2B cluster switches are _cs1_ and _cs2_.
* The nodes are _node1_ and _node2_.
* The cluster LIFs are _node1_clus1_ and _node1_clus2_ on node 1, and _node2_clus1_ and _node2_clus2_ on node 2 respectively.
* The `cluster1::*>` prompt indicates the name of the cluster.
* The cluster ports used in this procedure are _e3a_ and _e3b_.

.About this task

This procedure covers the following scenario:

* Switch C2 is replaced by switch cs2 first. 
** Shut down the ports to the cluster nodes. All ports must be shut down simultaneously to avoid cluster instability.
*** All cluster LIFs fail over to the new switch cs2.
** The cabling between the nodes and C2 is then disconnected from C2 and reconnected to cs2.

* Switch C1 is replaced by switch cs1.
** Shut down the ports to the cluster nodes. All ports must be shut down simultaneously to avoid cluster instability.
*** All cluster LIFs fail over to the new switch cs1. 
** The cabling between the nodes and C1 is then disconnected from C1 and reconnected to cs1.

NOTE: No operational inter-switch link (ISL) is needed during this procedure. This is by design because RCF version changes can affect ISL connectivity temporarily. To ensure non-disruptive cluster operations, the following procedure fails over all of the cluster LIFs to the operational partner switch while performing the steps on the target switch.

=== Step 1: Prepare for migration

. If AutoSupport is enabled on this cluster, suppress automatic case creation by invoking an AutoSupport message: 
+
`system node autosupport invoke -node * -type all -message MAINT=xh`
+
where _x_ is the duration of the maintenance window in hours.

. Change the privilege level to advanced, entering *y* when prompted to continue: 
+
`set -privilege advanced`
+
The advanced prompt (*>) appears.

. Disable auto-revert on the cluster LIFs. 
+
By disabling auto-revert for this procedure, the cluster LIFs will not automatically move back to their home port. They remain on the current port while it continues to be up and operational.
+
`network interface modify -vserver Cluster -lif * -auto-revert false`

=== Step 2: Configure ports and cabling

. Determine the administrative or operational status for each cluster interface.
+
Each port should display up for `Link` and `healthy` for `Health Status`.
+
.. Display the network port attributes: 
+
`network port show -ipspace Cluster`
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1
                                                                       Ignore
                                                 Speed(Mbps)  Health   Health
Port      IPspace    Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ---------- ---------------- ---- ----- ------------ -------- ------
e3a       Cluster    Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster    Cluster          up   9000  auto/100000  healthy  false

Node: node2
                                                                       Ignore
                                                 Speed(Mbps)  Health   Health
Port      IPspace    Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ---------- ---------------- ---- ----- ------------ -------- ------
e3a       Cluster    Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster    Cluster          up   9000  auto/100000  healthy  false
----
====

.. Display information about the LIFs and their designated home nodes: 
+
`network interface show -vserver Cluster`
+
Each LIF should display `up/up` for `Status Admin/Oper` and `true` for `Is Home`.
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
cluster1::*> *network interface show -vserver Cluster*

            Logical      Status     Network            Current     Current Is
Vserver     Interface    Admin/Oper Address/Mask       Node        Port    Home
----------- -----------  ---------- ------------------ ----------- ------- ----
Cluster
            node1_clus1  up/up      169.254.209.69/16  node1       e3a     true
            node1_clus2  up/up      169.254.49.125/16  node1       e3b     true
            node2_clus1  up/up      169.254.47.194/16  node2       e3a     true
            node2_clus2  up/up      169.254.19.183/16  node2       e3b     true

----
====

. The cluster ports on each node are connected to existing cluster switches in the following way (from the nodes' perspective) using the command: 
+
`network device-discovery show -protocol`
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
cluster1::*> *network device-discovery show -protocol cdp*
Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface         Platform
----------- ------ ------------------------- ----------------  ----------------
node1      /cdp
            e3a    C1 (6a:ad:4f:98:3b:3f)    0/1               -
            e3b    C2 (6a:ad:4f:98:4c:a4)    0/1               -
node2      /cdp
            e3a    C1 (6a:ad:4f:98:3b:3f)    0/2               -
            e3b    C2 (6a:ad:4f:98:4c:a4)    0/2               -
----
====

. The cluster ports and switches are connected in the following way (from the switches' perspective) using the command: 
+
`show cdp neighbors`
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
C1# *show cdp neighbors*

Capability Codes: R - Router, T - Trans-Bridge, B - Source-Route-Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater,
                  V - VoIP-Phone, D - Remotely-Managed-Device,
                  s - Supports-STP-Dispute

Device-ID             Local Intrfce Hldtme Capability  Platform         Port ID
node1                 Eth1/1        124    H           AFF-A400         e3a
node2                 Eth1/2        124    H           AFF-A400         e3a
C2                    0/13          179    S I s       CN1610           0/13
C2                    0/14          175    S I s       CN1610           0/14
C2                    0/15          179    S I s       CN1610           0/15
C2                    0/16          175    S I s       CN1610           0/16

C2# *show cdp neighbors*

Capability Codes: R - Router, T - Trans-Bridge, B - Source-Route-Bridge
                  S - Switch, H - Host, I - IGMP, r - Repeater,
                  V - VoIP-Phone, D - Remotely-Managed-Device,
                  s - Supports-STP-Dispute


Device-ID             Local Intrfce Hldtme Capability  Platform         Port ID
node1                 Eth1/1        124    H           AFF-A400         e3b
node2                 Eth1/2        124    H           AFF-A400         e3b
C1                    0/13          175    S I s       CN1610           0/13
C1                    0/14          175    S I s       CN1610           0/14
C1                    0/15          175    S I s       CN1610           0/15
C1                    0/16          175    S I s       CN1610           0/16
----
====

. Verify the connectivity of the remote cluster interfaces: 
+
// start of tabbed content

[role="tabbed-block"]

====

.ONTAP 9.9.1 and later

--
You can use the `network interface check cluster-connectivity` command to start an accessibility check for cluster connectivity and then display the details: 

`network interface check cluster-connectivity start` and `network interface check cluster-connectivity show`

[subs=+quotes]
----
cluster1::*> *network interface check cluster-connectivity start*
----

*NOTE:* Wait for a number of seconds before running the `show` command to display the details.


[subs=+quotes]
----
cluster1::*> *network interface check cluster-connectivity show*
                                  Source           Destination      Packet
Node   Date                       LIF              LIF              Loss
------ -------------------------- ---------------- ---------------- -----------
node1
       3/5/2022 19:21:18 -06:00   node1_clus2      node2-clus1      none
       3/5/2022 19:21:20 -06:00   node1_clus2      node2_clus2      none
node2
       3/5/2022 19:21:18 -06:00   node2_clus2      node1_clus1      none
       3/5/2022 19:21:20 -06:00   node2_clus2      node1_clus2      none
----
--

.All ONTAP releases
--
For all ONTAP releases, you can also use the `cluster ping-cluster -node <name>` command to check the connectivity:

`cluster ping-cluster -node <name>`


[subs=+quotes]
----
cluster1::*> *cluster ping-cluster -node local*
Host is node2
Getting addresses from network interface table...
Cluster node1_clus1 169.254.209.69 node1     e3a
Cluster node1_clus2 169.254.49.125 node1     e3b
Cluster node2_clus1 169.254.47.194 node2     e3a
Cluster node2_clus2 169.254.19.183 node2     e3b
Local = 169.254.47.194 169.254.19.183
Remote = 169.254.209.69 169.254.49.125
Cluster Vserver Id = 4294967293
Ping status:
....
Basic connectivity succeeds on 4 path(s)
Basic connectivity fails on 0 path(s)
................
Detected 9000 byte MTU on 4 path(s):
    Local 169.254.19.183 to Remote 169.254.209.69
    Local 169.254.19.183 to Remote 169.254.49.125
    Local 169.254.47.194 to Remote 169.254.209.69
    Local 169.254.47.194 to Remote 169.254.49.125
Larger than PMTU communication succeeds on 4 path(s)
RPC status:
2 paths up, 0 paths down (tcp check)
2 paths up, 0 paths down (udp check)
----
--
====

// end of tabbed content

[start=5]
. [[step5]] On switch C2, shut down the ports connected to the cluster ports of the nodes in order to fail over the cluster LIFs.
+
CAUTION: Do not attempt to manually migrate the cluster LIFs. 
+
[subs=+quotes]
----
(C2)# *configure*
(C2)(Config)# *interface 0/1-0/12*
(C2)(Interface 0/1-0/12)# *shutdown*
(C2)(Interface 0/1-0/12)# *exit*
(C2)(Config)# *exit*
----

. Move the node cluster ports from the old switch C2 to the new switch cs2, using appropriate cabling supported by Cisco 9332D-GX2B.

. Display the network port attributes: 
+
`network port show -ipspace Cluster`
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1
                                                                       Ignore
                                                 Speed(Mbps)  Health   Health
Port      IPspace    Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ---------- ---------------- ---- ----- ------------ -------- ------
e3a       Cluster    Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster    Cluster          up   9000  auto/100000  healthy  false

Node: node2
                                                                       Ignore
                                                 Speed(Mbps)  Health   Health
Port      IPspace    Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ---------- ---------------- ---- ----- ------------ -------- ------
e3a       Cluster    Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster    Cluster          up   9000  auto/100000  healthy  false
----
====

. The cluster ports on each node are now connected to cluster switches in the following way, from the nodes' perspective:
+
`network device-discovery show -protocol`
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
cluster1::*> *network device-discovery show -protocol cdp*

Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface         Platform
----------- ------ ------------------------- ----------------  ----------------
node1      /cdp
            e3a    C1  (6a:ad:4f:98:3b:3f)   0/1               CN1610
            e3b    cs2 (b8:ce:f6:19:1a:7e)   Ethernet1/1/1     N9K-C9336C-FX2
node2      /cdp
            e3a    C1  (6a:ad:4f:98:3b:3f)   0/2               CN1610
            e3b    cs2 (b8:ce:f6:19:1b:96)   Ethernet1/1/2     N9K-C9336C-FX2
----
====

. On switch cs2, verify that all node cluster ports are up: 
+
`network interface show -vserver Cluster`
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
cluster1::*> *network interface show -vserver Cluster*
            Logical      Status     Network            Current     Current Is
Vserver     Interfac     Admin/Oper Address/Mask       Node        Port    Home
----------- ------------ ---------- ------------------ ----------- ------- ----
Cluster
            node1_clus1  up/up      169.254.3.4/16     node1       e0b     false
            node1_clus2  up/up      169.254.3.5/16     node1       e0b     true
            node2_clus1  up/up      169.254.3.8/16     node2       e0b     false
            node2_clus2  up/up      169.254.3.9/16     node2       e0b     true
----
====

. On switch C1, shut down the ports connected to the cluster ports of the nodes in order to fail over the cluster LIFs.
+
[subs=+quotes]
----
(C1)# *configure*
(C1)(Config)# *interface 0/1-0/12*
(C1)(Interface 0/1-0/12)# *shutdown*
(C1)(Interface 0/1-0/12)# *exit*
(C1)(Config)# *exit*
----

. Move the node cluster ports from the old switch C1 to the new switch cs1, using appropriate cabling supported by Cisco 9332D-GX2B.

. Verify the final configuration of the cluster: 
+
`network port show -ipspace Cluster`
+
Each port should display `up` for `Link` and `healthy` for `Health Status`.
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1
                                                                       Ignore
                                                 Speed(Mbps)  Health   Health
Port      IPspace    Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ---------- ---------------- ---- ----- ------------ -------- ------
e3a       Cluster    Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster    Cluster          up   9000  auto/100000  healthy  false

Node: node2
                                                                       Ignore
                                                 Speed(Mbps)  Health   Health
Port      IPspace    Broadcast Domain Link MTU   Admin/Oper   Status   Status
--------- ---------- ---------------- ---- ----- ------------ -------- ------
e3a       Cluster    Cluster          up   9000  auto/100000  healthy  false
e3b       Cluster    Cluster          up   9000  auto/100000  healthy  false
----
====

. The cluster ports on each node are now connected to cluster switches in the following way, from the nodes' perspective:
+
`network device-discovery show -protocol`
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
cluster1::*> *network device-discovery show -protocol cdp*

Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface       Platform
----------- ------ ------------------------- --------------  ----------------
node1      /cdp
            e3a    cs1 (b8:ce:f6:19:1a:7e)   Ethernet1/1/1   N9K-C9336C-FX2
            e3b    cs2 (b8:ce:f6:19:1b:96)   Ethernet1/1/2   N9K-C9336C-FX2
node2      /cdp
            e3a    cs1 (b8:ce:f6:19:1a:7e)   Ethernet1/1/1   N9K-C9336C-FX2
            e3b    cs2 (b8:ce:f6:19:1b:96)   Ethernet1/1/2   N9K-C9336C-FX2
----
====

. On switches cs1 and cs2, verify that all node cluster ports are up: 
+
`network port show -ipspace Cluster`
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
cluster1::*> *network port show -ipspace Cluster*

Node: node1
                                                                       Ignore
                                                  Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status
--------- ------------ ---------------- ---- ---- ----------- -------- ------
e0a       Cluster      Cluster          up   9000  auto/10000 healthy  false
e0b       Cluster      Cluster          up   9000  auto/10000 healthy  false

Node: node2
                                                                       Ignore
                                                  Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status
--------- ------------ ---------------- ---- ---- ----------- -------- ------
e0a       Cluster      Cluster          up   9000  auto/10000 healthy  false
e0b       Cluster      Cluster          up   9000  auto/10000 healthy  false
----
====

. Verify that both nodes each have one connection to each switch: 
+
`network device-discovery show -protocol`
+
.Show example
[%collapsible]
====
The following example shows the appropriate results for both switches:

[subs=+quotes]
----
cluster1::*> *network device-discovery show -protocol cdp*
Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface       Platform
----------- ------ ------------------------- --------------  --------------
node1      /cdp
            e0a    cs1 (b8:ce:f6:19:1b:42)   Ethernet1/1/1   N9K-C9336C-FX2
            e0b    cs2 (b8:ce:f6:19:1b:96)   Ethernet1/1/2   N9K-C9336C-FX2

node2      /cdp
            e0a    cs1 (b8:ce:f6:19:1b:42)   Ethernet1/1/1   N9K-C9336C-FX2
            e0b    cs2 (b8:ce:f6:19:1b:96)   Ethernet1/1/2   N9K-C9336C-FX2
----
====

=== Step 3: Verify the configuration

. Enable auto-revert on the cluster LIFs: 
+
`cluster1::*> network interface modify -vserver Cluster -lif * -auto-revert true`

. Verify that all cluster network LIFs are back on their home ports: 
+
`network interface show`
+
.Show example
[%collapsible]
====
[subs=+quotes]
----
cluster1::*> *network interface show -vserver Cluster*

            Logical    Status     Network            Current       Current Is
Vserver     Interface  Admin/Oper Address/Mask       Node          Port    Home
----------- ---------- ---------- ------------------ ------------- ------- ----
Cluster
            node1_clus1  up/up    169.254.209.69/16  node1         e3a     true
            node1_clus2  up/up    169.254.49.125/16  node1         e3b     true
            node2_clus1  up/up    169.254.47.194/16  node2         e3a     true
            node2_clus2  up/up    169.254.19.183/16  node2         e3b     true
----
====

. Change the privilege level back to admin: 
+
`set -privilege admin`

. If you suppressed automatic case creation, re-enable it by invoking an AutoSupport message: 
+
`system node autosupport invoke -node * -type all -message MAINT=END`

.What's next?

link:../switch-cshm/config-overview.html[Configure switch health monitoring].

// New content for OAM project, AFFFASDOC-331, 2025-MAY-08